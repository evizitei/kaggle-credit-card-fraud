{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud\n",
    "\n",
    "This notebook uses data from kaggle to practice supervised learning on pre-PCA'd data.  The relevant dataset can be found [here](https://www.kaggle.com/dalpozz/creditcardfraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "frame = pd.read_csv(\"./data/creditcard.csv\")\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial impressions\n",
    "\n",
    "The data in this csv doesn't represent recognizable signals from the relevant transactions; each \"V\" is already a PCA vector, and is intentionally provided without the original dataset for confidentiality reasons.  This means I have no way to develop any intuition about what's important just from knowing the domain, which is kind of a neat new challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.165980e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.373150e-15</td>\n",
       "      <td>2.086869e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.490107e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.177556e-16</td>\n",
       "      <td>-2.406455e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.656562e-16</td>\n",
       "      <td>-3.444850e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.471968e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.687098e-15</td>\n",
       "      <td>-3.666453e-16</td>\n",
       "      <td>-1.220404e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.165980e-15  3.416908e-16 -1.373150e-15  2.086869e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.490107e-15 -5.556467e-16  1.177556e-16 -2.406455e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...        1.656562e-16 -3.444850e-16  2.578648e-16  4.471968e-15   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.687098e-15 -3.666453e-16 -1.220404e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There *are* two columns that are labeled in the dataset, one is time since the interval began for this dataset, and one is the value of the transaction.  Let's take a minute and see if either of them are noticably different from fraudulent to regular transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     492.000000\n",
       "mean      122.211321\n",
       "std       256.683288\n",
       "min         0.000000\n",
       "25%         1.000000\n",
       "50%         9.250000\n",
       "75%       105.890000\n",
       "max      2125.870000\n",
       "Name: Amount, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.Amount[frame.Class == 1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    284315.000000\n",
       "mean         88.291022\n",
       "std         250.105092\n",
       "min           0.000000\n",
       "25%           5.650000\n",
       "50%          22.000000\n",
       "75%          77.050000\n",
       "max       25691.160000\n",
       "Name: Amount, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.Amount[frame.Class == 0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For amount, unsurprisingly, there's a lot less range for fraudulent transactions.  I suppose to a fraudster, it feels like large transactions are likely to get noticed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       492.000000\n",
       "mean      80746.806911\n",
       "std       47835.365138\n",
       "min         406.000000\n",
       "25%       41241.500000\n",
       "50%       75568.500000\n",
       "75%      128483.000000\n",
       "max      170348.000000\n",
       "Name: Time, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.Time[frame.Class == 1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    284315.000000\n",
       "mean      94838.202258\n",
       "std       47484.015786\n",
       "min           0.000000\n",
       "25%       54230.000000\n",
       "50%       84711.000000\n",
       "75%      139333.000000\n",
       "max      172792.000000\n",
       "Name: Time, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.Time[frame.Class == 0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance there isn't much interesting here for time. However, the categories are very unbalanced; there are 284,000 valid transactions and just under 500 fraudulent.  It would be very possible in a test train split to accidentally put all the fraud examples in one set or the other.  We should split them and control how many of each go into each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud = frame[frame.Class == 1]\n",
    "valid = frame[frame.Class == 0]\n",
    "len(fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_percentage = 0.85\n",
    "validation_percentage_of_test = 0.3\n",
    "\n",
    "fraud_train = fraud.sample(frac=training_percentage)\n",
    "fraud_train_inverse = fraud.drop(fraud_train.index)\n",
    "fraud_validation = fraud_train_inverse.sample(frac=validation_percentage_of_test)\n",
    "fraud_test = fraud_train_inverse.drop(fraud_validation.index)\n",
    "# oversample the minority class\n",
    "fraud_train = pd.concat([fraud_train, fraud_train, fraud_train, fraud_train])\n",
    "fraud_train = pd.concat([fraud_train, fraud_train, fraud_train, fraud_train])\n",
    "fraud_train = pd.concat([fraud_train, fraud_train, fraud_train, fraud_train])\n",
    "\n",
    "\n",
    "valid_train = valid.sample(frac=training_percentage)\n",
    "valid_train_inverse = valid.drop(valid_train.index)\n",
    "valid_validation = valid_train_inverse.sample(frac=validation_percentage_of_test)\n",
    "valid_test = valid_train_inverse.drop(valid_validation.index)\n",
    "# undersample the majority class\n",
    "valid_train = valid_train.sample(frac=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now they can be concatenated into complete sets and shuffled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train_features = shuffle(pd.concat([fraud_train, valid_train]))\n",
    "test_features = shuffle(pd.concat([fraud_test, valid_test]))\n",
    "validation_features = shuffle(pd.concat([fraud_validation, valid_validation]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pull out the labels, and drop them from the feature frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels = train_features.Class\n",
    "test_labels = test_features.Class\n",
    "validation_labels = validation_features.Class\n",
    "train_features = train_features.drop(['Class'], axis=1)\n",
    "test_features = test_features.drop(['Class'], axis=1)\n",
    "validation_features = validation_features.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "      <td>147586.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>92083.879081</td>\n",
       "      <td>-0.822654</td>\n",
       "      <td>0.631290</td>\n",
       "      <td>-1.221204</td>\n",
       "      <td>0.805675</td>\n",
       "      <td>-0.526940</td>\n",
       "      <td>-0.253155</td>\n",
       "      <td>-0.957899</td>\n",
       "      <td>0.093502</td>\n",
       "      <td>-0.450655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065634</td>\n",
       "      <td>0.112510</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>-0.010763</td>\n",
       "      <td>-0.019136</td>\n",
       "      <td>0.007926</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>0.029891</td>\n",
       "      <td>0.015904</td>\n",
       "      <td>93.021534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47934.843600</td>\n",
       "      <td>3.806442</td>\n",
       "      <td>2.705999</td>\n",
       "      <td>4.197309</td>\n",
       "      <td>2.456529</td>\n",
       "      <td>2.813501</td>\n",
       "      <td>1.532080</td>\n",
       "      <td>3.825485</td>\n",
       "      <td>3.017389</td>\n",
       "      <td>1.735221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922374</td>\n",
       "      <td>1.715664</td>\n",
       "      <td>0.900441</td>\n",
       "      <td>0.894272</td>\n",
       "      <td>0.590721</td>\n",
       "      <td>0.580118</td>\n",
       "      <td>0.478784</td>\n",
       "      <td>0.681384</td>\n",
       "      <td>0.376441</td>\n",
       "      <td>248.840715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>-46.855047</td>\n",
       "      <td>-63.344698</td>\n",
       "      <td>-32.965346</td>\n",
       "      <td>-5.519697</td>\n",
       "      <td>-42.147898</td>\n",
       "      <td>-23.496714</td>\n",
       "      <td>-43.557242</td>\n",
       "      <td>-73.216718</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.009635</td>\n",
       "      <td>-34.830382</td>\n",
       "      <td>-10.933144</td>\n",
       "      <td>-30.269720</td>\n",
       "      <td>-2.822684</td>\n",
       "      <td>-7.495741</td>\n",
       "      <td>-2.534330</td>\n",
       "      <td>-22.565679</td>\n",
       "      <td>-11.710896</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>51426.250000</td>\n",
       "      <td>-1.298443</td>\n",
       "      <td>-0.477496</td>\n",
       "      <td>-1.615401</td>\n",
       "      <td>-0.685276</td>\n",
       "      <td>-0.934655</td>\n",
       "      <td>-0.971244</td>\n",
       "      <td>-0.892838</td>\n",
       "      <td>-0.209479</td>\n",
       "      <td>-1.026896</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.206396</td>\n",
       "      <td>-0.215443</td>\n",
       "      <td>-0.545816</td>\n",
       "      <td>-0.182292</td>\n",
       "      <td>-0.372216</td>\n",
       "      <td>-0.317991</td>\n",
       "      <td>-0.314087</td>\n",
       "      <td>-0.069208</td>\n",
       "      <td>-0.054008</td>\n",
       "      <td>3.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>83243.500000</td>\n",
       "      <td>-0.272500</td>\n",
       "      <td>0.250864</td>\n",
       "      <td>-0.168438</td>\n",
       "      <td>0.285916</td>\n",
       "      <td>-0.148289</td>\n",
       "      <td>-0.388440</td>\n",
       "      <td>-0.083415</td>\n",
       "      <td>0.057332</td>\n",
       "      <td>-0.201091</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039046</td>\n",
       "      <td>0.012646</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>-0.016927</td>\n",
       "      <td>0.028828</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>-0.039680</td>\n",
       "      <td>0.014370</td>\n",
       "      <td>0.017171</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>137769.500000</td>\n",
       "      <td>1.233204</td>\n",
       "      <td>1.145381</td>\n",
       "      <td>0.855995</td>\n",
       "      <td>1.384394</td>\n",
       "      <td>0.579607</td>\n",
       "      <td>0.296348</td>\n",
       "      <td>0.493810</td>\n",
       "      <td>0.459822</td>\n",
       "      <td>0.478908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215385</td>\n",
       "      <td>0.278251</td>\n",
       "      <td>0.539630</td>\n",
       "      <td>0.161528</td>\n",
       "      <td>0.410870</td>\n",
       "      <td>0.368708</td>\n",
       "      <td>0.272505</td>\n",
       "      <td>0.169662</td>\n",
       "      <td>0.111651</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172788.000000</td>\n",
       "      <td>2.454930</td>\n",
       "      <td>22.057729</td>\n",
       "      <td>9.382558</td>\n",
       "      <td>16.715537</td>\n",
       "      <td>34.099309</td>\n",
       "      <td>22.529298</td>\n",
       "      <td>36.877368</td>\n",
       "      <td>20.007208</td>\n",
       "      <td>15.594995</td>\n",
       "      <td>...</td>\n",
       "      <td>39.420904</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>10.503090</td>\n",
       "      <td>19.228169</td>\n",
       "      <td>4.022866</td>\n",
       "      <td>7.519589</td>\n",
       "      <td>3.220178</td>\n",
       "      <td>12.152401</td>\n",
       "      <td>22.620072</td>\n",
       "      <td>19656.530000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time             V1             V2             V3  \\\n",
       "count  147586.000000  147586.000000  147586.000000  147586.000000   \n",
       "mean    92083.879081      -0.822654       0.631290      -1.221204   \n",
       "std     47934.843600       3.806442       2.705999       4.197309   \n",
       "min         2.000000     -46.855047     -63.344698     -32.965346   \n",
       "25%     51426.250000      -1.298443      -0.477496      -1.615401   \n",
       "50%     83243.500000      -0.272500       0.250864      -0.168438   \n",
       "75%    137769.500000       1.233204       1.145381       0.855995   \n",
       "max    172788.000000       2.454930      22.057729       9.382558   \n",
       "\n",
       "                  V4             V5             V6             V7  \\\n",
       "count  147586.000000  147586.000000  147586.000000  147586.000000   \n",
       "mean        0.805675      -0.526940      -0.253155      -0.957899   \n",
       "std         2.456529       2.813501       1.532080       3.825485   \n",
       "min        -5.519697     -42.147898     -23.496714     -43.557242   \n",
       "25%        -0.685276      -0.934655      -0.971244      -0.892838   \n",
       "50%         0.285916      -0.148289      -0.388440      -0.083415   \n",
       "75%         1.384394       0.579607       0.296348       0.493810   \n",
       "max        16.715537      34.099309      22.529298      36.877368   \n",
       "\n",
       "                  V8             V9      ...                  V20  \\\n",
       "count  147586.000000  147586.000000      ...        147586.000000   \n",
       "mean        0.093502      -0.450655      ...             0.065634   \n",
       "std         3.017389       1.735221      ...             0.922374   \n",
       "min       -73.216718     -13.434066      ...           -28.009635   \n",
       "25%        -0.209479      -1.026896      ...            -0.206396   \n",
       "50%         0.057332      -0.201091      ...            -0.039046   \n",
       "75%         0.459822       0.478908      ...             0.215385   \n",
       "max        20.007208      15.594995      ...            39.420904   \n",
       "\n",
       "                 V21            V22            V23            V24  \\\n",
       "count  147586.000000  147586.000000  147586.000000  147586.000000   \n",
       "mean        0.112510       0.002233      -0.010763      -0.019136   \n",
       "std         1.715664       0.900441       0.894272       0.590721   \n",
       "min       -34.830382     -10.933144     -30.269720      -2.822684   \n",
       "25%        -0.215443      -0.545816      -0.182292      -0.372216   \n",
       "50%         0.012646       0.005741      -0.016927       0.028828   \n",
       "75%         0.278251       0.539630       0.161528       0.410870   \n",
       "max        27.202839      10.503090      19.228169       4.022866   \n",
       "\n",
       "                 V25            V26            V27            V28  \\\n",
       "count  147586.000000  147586.000000  147586.000000  147586.000000   \n",
       "mean        0.007926       0.010054       0.029891       0.015904   \n",
       "std         0.580118       0.478784       0.681384       0.376441   \n",
       "min        -7.495741      -2.534330     -22.565679     -11.710896   \n",
       "25%        -0.317991      -0.314087      -0.069208      -0.054008   \n",
       "50%         0.030500      -0.039680       0.014370       0.017171   \n",
       "75%         0.368708       0.272505       0.169662       0.111651   \n",
       "max         7.519589       3.220178      12.152401      22.620072   \n",
       "\n",
       "              Amount  \n",
       "count  147586.000000  \n",
       "mean       93.021534  \n",
       "std       248.840715  \n",
       "min         0.000000  \n",
       "25%         3.790000  \n",
       "50%        20.000000  \n",
       "75%        84.000000  \n",
       "max     19656.530000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale features so they're all of comparable varience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = train_features.columns.values\n",
    "\n",
    "for col_name in cols:\n",
    "    avg, dev = frame[col_name].mean(), frame[col_name].std()\n",
    "    train_features.loc[:, col_name] = (train_features[col_name] - avg) / dev\n",
    "    train_features.loc[:, col_name] = (train_features[col_name] - avg) / dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "trainX = train_features.as_matrix()\n",
    "testX = test_features.as_matrix()\n",
    "validX = validation_features.as_matrix()\n",
    "\n",
    "trainY = np.reshape(train_labels.as_matrix(), (-1, 1))\n",
    "testY = np.reshape(test_labels.as_matrix(), (-1, 1))\n",
    "validY = np.reshape(validation_labels.as_matrix(), (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# layer size configuration\n",
    "input_nodes = len(trainX[0])\n",
    "hidden_layer_1 = 200\n",
    "hidden_layer_2 = 50\n",
    "hidden_layer_3 = 13\n",
    "output_nodes = 1\n",
    "dropout_keep_percentage = 0.75\n",
    "learning_rate = 0.01\n",
    "training_epochs = 2000\n",
    "display_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x_inputs = tf.placeholder(tf.float32, [None, input_nodes], name=\"x_inputs\")\n",
    "y_outputs = tf.placeholder(tf.float32,[None,1], name=\"y_outputs\")\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "weights1 = tf.Variable(tf.truncated_normal([input_nodes, hidden_layer_1], stddev=0.1))\n",
    "bias1 = tf.Variable(tf.zeros([hidden_layer_1]))\n",
    "hidden1 = tf.nn.relu(tf.matmul(x_inputs, weights1) + bias1)\n",
    "\n",
    "weights2 = tf.Variable(tf.truncated_normal([hidden_layer_1, hidden_layer_2], stddev=0.1))\n",
    "bias2 = tf.Variable(tf.zeros([hidden_layer_2]))\n",
    "hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + bias2)\n",
    "\n",
    "weights3 = tf.Variable(tf.truncated_normal([hidden_layer_2, hidden_layer_3], stddev=0.1))\n",
    "bias3 = tf.Variable(tf.zeros([hidden_layer_3]))\n",
    "hidden3 = tf.nn.relu(tf.matmul(hidden2, weights3) + bias3)\n",
    "hidden3 = tf.nn.dropout(hidden3, keep_prob)\n",
    "\n",
    "output_weights = tf.Variable(tf.truncated_normal([hidden_layer_3, output_nodes], stddev=0.1))\n",
    "output_bias = tf.Variable(tf.zeros([output_nodes]))\n",
    "output = tf.nn.sigmoid(tf.matmul(hidden3, output_weights) + output_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(output - y_outputs))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "correct_prediction = tf.equal(tf.round(output), tf.round(y_outputs))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-74-80f4a80570a7>:1: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 ...TRAINING: 0.568279\n",
      "COST: 0.247112\n",
      "TEST: [0.99789333]\n",
      "epoch: 1 ...2 ...3 ...4 ...5 ...6 ...7 ...8 ...9 ...10 ...11 ...12 ...13 ...14 ...15 ...16 ...17 ...18 ...19 ...20 ...21 ...22 ...23 ...24 ...25 ...26 ...27 ...28 ...29 ...30 ...31 ...32 ...33 ...34 ...35 ...36 ...37 ...38 ...39 ...40 ...41 ...42 ...43 ...44 ...45 ...46 ...47 ...48 ...49 ...50 ...51 ...52 ...53 ...54 ...55 ...56 ...57 ...58 ...59 ...60 ...61 ...62 ...63 ...64 ...65 ...66 ...67 ...68 ...69 ...70 ...71 ...72 ...73 ...74 ...75 ...76 ...77 ...78 ...79 ...80 ...81 ...82 ...83 ...84 ...85 ...86 ...87 ...88 ...89 ...90 ...91 ...92 ...93 ...94 ...95 ...96 ...97 ...98 ...99 ...100 ...TRAINING: 0.900953\n",
      "COST: 0.219499\n",
      "TEST: [0.99792677]\n",
      "epoch: 101 ...102 ...103 ...104 ...105 ...106 ...107 ...108 ...109 ...110 ...111 ...112 ...113 ...114 ...115 ...116 ...117 ...118 ...119 ...120 ...121 ...122 ...123 ...124 ...125 ...126 ...127 ...128 ...129 ...130 ...131 ...132 ...133 ...134 ...135 ...136 ...137 ...138 ...139 ...140 ...141 ...142 ...143 ...144 ...145 ...146 ...147 ...148 ...149 ...150 ...151 ...152 ...153 ...154 ...155 ...156 ...157 ...158 ...159 ...160 ...161 ...162 ...163 ...164 ...165 ...166 ...167 ...168 ...169 ...170 ...171 ...172 ...173 ...174 ...175 ...176 ...177 ...178 ...179 ...180 ...181 ...182 ...183 ...184 ...185 ...186 ...187 ...188 ...189 ...190 ...191 ...192 ...193 ...194 ...195 ...196 ...197 ...198 ...199 ...200 ...TRAINING: 0.900844\n",
      "COST: 0.194804\n",
      "TEST: [0.99789333]\n",
      "epoch: 201 ...202 ...203 ...204 ...205 ...206 ...207 ...208 ...209 ...210 ...211 ...212 ...213 ...214 ...215 ...216 ...217 ...218 ...219 ...220 ...221 ...222 ...223 ...224 ...225 ...226 ...227 ...228 ...229 ...230 ...231 ...232 ...233 ...234 ...235 ...236 ...237 ...238 ...239 ...240 ...241 ...242 ...243 ...244 ...245 ...246 ...247 ...248 ...249 ...250 ...251 ...252 ...253 ...254 ...255 ...256 ...257 ...258 ...259 ...260 ...261 ...262 ...263 ...264 ...265 ...266 ...267 ...268 ...269 ...270 ...271 ...272 ...273 ...274 ...275 ...276 ...277 ...278 ...279 ...280 ...281 ...282 ...283 ...284 ...285 ...286 ...287 ...288 ...289 ...290 ...291 ...292 ...293 ...294 ...295 ...296 ...297 ...298 ...299 ...300 ...TRAINING: 0.921666\n",
      "COST: 0.171304\n",
      "TEST: [0.99782646]\n",
      "epoch: 301 ...302 ...303 ...304 ...305 ...306 ...307 ...308 ...309 ...310 ...311 ...312 ...313 ...314 ...315 ...316 ...317 ...318 ...319 ...320 ...321 ...322 ...323 ...324 ...325 ...326 ...327 ...328 ...329 ...330 ...331 ...332 ...333 ...334 ...335 ...336 ...337 ...338 ...339 ...340 ...341 ...342 ...343 ...344 ...345 ...346 ...347 ...348 ...349 ...350 ...351 ...352 ...353 ...354 ...355 ...356 ...357 ...358 ...359 ...360 ...361 ...362 ...363 ...364 ...365 ...366 ...367 ...368 ...369 ...370 ...371 ...372 ...373 ...374 ...375 ...376 ...377 ...378 ...379 ...380 ...381 ...382 ...383 ...384 ...385 ...386 ...387 ...388 ...389 ...390 ...391 ...392 ...393 ...394 ...395 ...396 ...397 ...398 ...399 ...400 ...TRAINING: 0.934255\n",
      "COST: 0.149308\n",
      "TEST: [0.99772614]\n",
      "epoch: 401 ...402 ...403 ...404 ...405 ...406 ...407 ...408 ...409 ...410 ...411 ...412 ...413 ...414 ...415 ...416 ...417 ...418 ...419 ...420 ...421 ...422 ...423 ...424 ...425 ...426 ...427 ...428 ...429 ...430 ...431 ...432 ...433 ...434 ...435 ...436 ...437 ...438 ...439 ...440 ...441 ...442 ...443 ...444 ...445 ...446 ...447 ...448 ...449 ...450 ...451 ...452 ...453 ...454 ...455 ...456 ...457 ...458 ...459 ...460 ...461 ...462 ...463 ...464 ...465 ...466 ...467 ...468 ...469 ...470 ...471 ...472 ...473 ...474 ...475 ...476 ...477 ...478 ...479 ...480 ...481 ...482 ...483 ...484 ...485 ...486 ...487 ...488 ...489 ...490 ...491 ...492 ...493 ...494 ...495 ...496 ...497 ...498 ...499 ...500 ...TRAINING: 0.944676\n",
      "COST: 0.129756\n",
      "TEST: [0.99765927]\n",
      "epoch: 501 ...502 ...503 ...504 ...505 ...506 ...507 ...508 ...509 ...510 ...511 ...512 ...513 ...514 ...515 ...516 ...517 ...518 ...519 ...520 ...521 ...522 ...523 ...524 ...525 ...526 ...527 ...528 ...529 ...530 ...531 ...532 ...533 ...534 ...535 ...536 ...537 ...538 ...539 ...540 ...541 ...542 ...543 ...544 ...545 ...546 ...547 ...548 ...549 ...550 ...551 ...552 ...553 ...554 ...555 ...556 ...557 ...558 ...559 ...560 ...561 ...562 ...563 ...564 ...565 ...566 ...567 ...568 ...569 ...570 ...571 ...572 ...573 ...574 ...575 ...576 ...577 ...578 ...579 ...580 ...581 ...582 ...583 ...584 ...585 ...586 ...587 ...588 ...589 ...590 ...591 ...592 ...593 ...594 ...595 ...596 ...597 ...598 ...599 ...600 ...TRAINING: 0.950795\n",
      "COST: 0.112752\n",
      "TEST: [0.99729145]\n",
      "epoch: 601 ...602 ...603 ...604 ...605 ...606 ...607 ...608 ...609 ...610 ...611 ...612 ...613 ...614 ...615 ...616 ...617 ...618 ...619 ...620 ...621 ...622 ...623 ...624 ...625 ...626 ...627 ...628 ...629 ...630 ...631 ...632 ...633 ...634 ...635 ...636 ...637 ...638 ...639 ...640 ...641 ...642 ...643 ...644 ...645 ...646 ...647 ...648 ...649 ...650 ...651 ...652 ...653 ...654 ...655 ...656 ...657 ...658 ...659 ...660 ...661 ...662 ...663 ...664 ...665 ...666 ...667 ...668 ...669 ...670 ...671 ...672 ...673 ...674 ...675 ...676 ...677 ...678 ...679 ...680 ...681 ...682 ...683 ...684 ...685 ...686 ...687 ...688 ...689 ...690 ...691 ...692 ...693 ...694 ...695 ...696 ...697 ...698 ...699 ...700 ...TRAINING: 0.953912\n",
      "COST: 0.0981554\n",
      "TEST: [0.98980105]\n",
      "epoch: 701 ...702 ...703 ...704 ...705 ...706 ...707 ...708 ...709 ...710 ...711 ...712 ...713 ...714 ...715 ...716 ...717 ...718 ...719 ...720 ...721 ...722 ...723 ...724 ...725 ...726 ...727 ...728 ...729 ...730 ...731 ...732 ...733 ...734 ...735 ...736 ...737 ...738 ...739 ...740 ...741 ...742 ...743 ...744 ...745 ...746 ...747 ...748 ...749 ...750 ...751 ...752 ...753 ...754 ...755 ...756 ...757 ...758 ...759 ...760 ...761 ...762 ...763 ...764 ...765 ...766 ...767 ...768 ...769 ...770 ...771 ...772 ...773 ...774 ...775 ...776 ...777 ...778 ...779 ...780 ...781 ...782 ...783 ...784 ...785 ...786 ...787 ...788 ...789 ...790 ...791 ...792 ...793 ...794 ...795 ...796 ...797 ...798 ...799 ...800 ...TRAINING: 0.955741\n",
      "COST: 0.0858025\n",
      "TEST: [0.0029426517]\n",
      "epoch: 801 ...802 ...803 ...804 ...805 ...806 ...807 ...808 ...809 ...810 ...811 ...812 ...813 ...814 ...815 ...816 ...817 ...818 ...819 ...820 ...821 ...822 ...823 ...824 ...825 ...826 ...827 ...828 ...829 ...830 ...831 ...832 ...833 ...834 ...835 ...836 ...837 ...838 ...839 ...840 ...841 ...842 ...843 ...844 ...845 ...846 ...847 ...848 ...849 ...850 ...851 ...852 ...853 ...854 ...855 ...856 ...857 ...858 ...859 ...860 ...861 ...862 ...863 ...864 ...865 ...866 ...867 ...868 ...869 ...870 ...871 ...872 ...873 ...874 ...875 ...876 ...877 ...878 ...879 ...880 ...881 ...882 ...883 ...884 ...885 ...886 ...887 ...888 ...889 ...890 ...891 ...892 ...893 ...894 ...895 ...896 ...897 ...898 ...899 ...900 ...TRAINING: 0.955795\n",
      "COST: 0.0754635\n",
      "TEST: [0.002240428]\n",
      "epoch: 901 ...902 ...903 ...904 ...905 ...906 ...907 ...908 ...909 ...910 ...911 ...912 ...913 ...914 ...915 ...916 ...917 ...918 ...919 ...920 ...921 ...922 ...923 ...924 ...925 ...926 ...927 ...928 ...929 ...930 ...931 ...932 ...933 ...934 ...935 ...936 ...937 ...938 ...939 ...940 ...941 ...942 ...943 ...944 ...945 ...946 ...947 ...948 ...949 ...950 ...951 ...952 ...953 ...954 ...955 ...956 ...957 ...958 ...959 ...960 ...961 ...962 ...963 ...964 ...965 ...966 ...967 ...968 ...969 ...970 ...971 ...972 ...973 ...974 ...975 ...976 ...977 ...978 ...979 ...980 ...981 ...982 ...983 ...984 ...985 ...986 ...987 ...988 ...989 ...990 ...991 ...992 ...993 ...994 ...995 ...996 ...997 ...998 ...999 ...1000 ...TRAINING: 0.957557\n",
      "COST: 0.0669156\n",
      "TEST: [0.0020732319]\n",
      "epoch: 1001 ...1002 ...1003 ...1004 ...1005 ...1006 ...1007 ...1008 ...1009 ...1010 ...1011 ...1012 ...1013 ...1014 ...1015 ...1016 ...1017 ...1018 ...1019 ...1020 ...1021 ...1022 ...1023 ...1024 ...1025 ...1026 ...1027 ...1028 ...1029 ...1030 ...1031 ...1032 ...1033 ...1034 ...1035 ...1036 ...1037 ...1038 ...1039 ...1040 ...1041 ...1042 ...1043 ...1044 ...1045 ...1046 ...1047 ...1048 ...1049 ...1050 ...1051 ...1052 ...1053 ...1054 ...1055 ...1056 ...1057 ...1058 ...1059 ...1060 ...1061 ...1062 ...1063 ...1064 ...1065 ...1066 ...1067 ...1068 ...1069 ...1070 ...1071 ...1072 ...1073 ...1074 ...1075 ...1076 ...1077 ...1078 ...1079 ...1080 ...1081 ...1082 ...1083 ...1084 ...1085 ...1086 ...1087 ...1088 ...1089 ...1090 ...1091 ...1092 ...1093 ...1094 ...1095 ...1096 ...1097 ...1098 ...1099 ...1100 ...TRAINING: 0.958892\n",
      "COST: 0.0599854\n",
      "TEST: [0.0020397927]\n",
      "epoch: 1101 ...1102 ...1103 ...1104 ...1105 ...1106 ...1107 ...1108 ...1109 ...1110 ...1111 ...1112 ...1113 ...1114 ...1115 ...1116 ...1117 ...1118 ...1119 ...1120 ...1121 ...1122 ...1123 ...1124 ...1125 ...1126 ...1127 ...1128 ...1129 ...1130 ...1131 ...1132 ...1133 ...1134 ...1135 ...1136 ...1137 ...1138 ...1139 ...1140 ...1141 ...1142 ...1143 ...1144 ...1145 ...1146 ...1147 ...1148 ...1149 ...1150 ...1151 ...1152 ...1153 ...1154 ...1155 ...1156 ...1157 ...1158 ...1159 ...1160 ...1161 ...1162 ...1163 ...1164 ...1165 ...1166 ...1167 ...1168 ...1169 ...1170 ...1171 ...1172 ...1173 ...1174 ...1175 ...1176 ...1177 ...1178 ...1179 ...1180 ...1181 ...1182 ...1183 ...1184 ...1185 ...1186 ...1187 ...1188 ...1189 ...1190 ...1191 ...1192 ...1193 ...1194 ...1195 ...1196 ...1197 ...1198 ...1199 ...1200 ...TRAINING: 0.960227\n",
      "COST: 0.0543922\n",
      "TEST: [0.001939475]\n",
      "epoch: 1201 ...1202 ...1203 ...1204 ...1205 ...1206 ...1207 ...1208 ...1209 ...1210 ...1211 ...1212 ...1213 ...1214 ...1215 ...1216 ...1217 ...1218 ...1219 ...1220 ...1221 ...1222 ...1223 ...1224 ...1225 ...1226 ...1227 ...1228 ...1229 ...1230 ...1231 ...1232 ...1233 ...1234 ...1235 ...1236 ...1237 ...1238 ...1239 ...1240 ...1241 ...1242 ...1243 ...1244 ...1245 ...1246 ...1247 ...1248 ...1249 ...1250 ...1251 ...1252 ...1253 ...1254 ...1255 ...1256 ...1257 ...1258 ...1259 ...1260 ...1261 ...1262 ...1263 ...1264 ...1265 ...1266 ...1267 ...1268 ...1269 ...1270 ...1271 ...1272 ...1273 ...1274 ...1275 ...1276 ...1277 ...1278 ...1279 ...1280 ...1281 ...1282 ...1283 ...1284 ...1285 ...1286 ...1287 ...1288 ...1289 ...1290 ...1291 ...1292 ...1293 ...1294 ...1295 ...1296 ...1297 ...1298 ...1299 ...1300 ...TRAINING: 0.960254\n",
      "COST: 0.0498374\n",
      "TEST: [0.0019060358]\n",
      "epoch: 1301 ...1302 ...1303 ...1304 ...1305 ...1306 ...1307 ...1308 ...1309 ...1310 ...1311 ...1312 ...1313 ...1314 ...1315 ...1316 ...1317 ...1318 ...1319 ...1320 ...1321 ...1322 ...1323 ...1324 ...1325 ...1326 ...1327 ...1328 ...1329 ...1330 ...1331 ...1332 ...1333 ...1334 ...1335 ...1336 ...1337 ...1338 ...1339 ...1340 ...1341 ...1342 ...1343 ...1344 ...1345 ...1346 ...1347 ...1348 ...1349 ...1350 ...1351 ...1352 ...1353 ...1354 ...1355 ...1356 ...1357 ...1358 ...1359 ...1360 ...1361 ...1362 ...1363 ...1364 ...1365 ...1366 ...1367 ...1368 ...1369 ...1370 ...1371 ...1372 ...1373 ...1374 ...1375 ...1376 ...1377 ...1378 ...1379 ...1380 ...1381 ...1382 ...1383 ...1384 ...1385 ...1386 ...1387 ...1388 ...1389 ...1390 ...1391 ...1392 ...1393 ...1394 ...1395 ...1396 ...1397 ...1398 ...1399 ...1400 ...TRAINING: 0.961561\n",
      "COST: 0.0461121\n",
      "TEST: [0.0018725966]\n",
      "epoch: 1401 ...1402 ...1403 ...1404 ...1405 ...1406 ...1407 ...1408 ...1409 ...1410 ...1411 ...1412 ...1413 ...1414 ...1415 ...1416 ...1417 ...1418 ...1419 ...1420 ...1421 ...1422 ...1423 ...1424 ...1425 ...1426 ...1427 ...1428 ...1429 ...1430 ...1431 ...1432 ...1433 ...1434 ...1435 ...1436 ...1437 ...1438 ...1439 ...1440 ...1441 ...1442 ...1443 ...1444 ...1445 ...1446 ...1447 ...1448 ...1449 ...1450 ...1451 ...1452 ...1453 ...1454 ...1455 ...1456 ...1457 ...1458 ...1459 ...1460 ...1461 ...1462 ...1463 ...1464 ...1465 ...1466 ...1467 ...1468 ...1469 ...1470 ...1471 ...1472 ...1473 ...1474 ...1475 ...1476 ...1477 ...1478 ...1479 ...1480 ...1481 ...1482 ...1483 ...1484 ...1485 ...1486 ...1487 ...1488 ...1489 ...1490 ...1491 ...1492 ...1493 ...1494 ...1495 ...1496 ...1497 ...1498 ...1499 ...1500 ...TRAINING: 0.962422\n",
      "COST: 0.0430481\n",
      "TEST: [0.0018391573]\n",
      "epoch: 1501 ...1502 ...1503 ...1504 ...1505 ...1506 ...1507 ...1508 ...1509 ...1510 ...1511 ...1512 ...1513 ...1514 ...1515 ...1516 ...1517 ...1518 ...1519 ...1520 ...1521 ...1522 ...1523 ...1524 ...1525 ...1526 ...1527 ...1528 ...1529 ...1530 ...1531 ...1532 ...1533 ...1534 ...1535 ...1536 ...1537 ...1538 ...1539 ...1540 ...1541 ...1542 ...1543 ...1544 ...1545 ...1546 ...1547 ...1548 ...1549 ...1550 ...1551 ...1552 ...1553 ...1554 ...1555 ...1556 ...1557 ...1558 ...1559 ...1560 ...1561 ...1562 ...1563 ...1564 ...1565 ...1566 ...1567 ...1568 ...1569 ...1570 ...1571 ...1572 ...1573 ...1574 ...1575 ...1576 ...1577 ...1578 ...1579 ...1580 ...1581 ...1582 ...1583 ...1584 ...1585 ...1586 ...1587 ...1588 ...1589 ...1590 ...1591 ...1592 ...1593 ...1594 ...1595 ...1596 ...1597 ...1598 ...1599 ...1600 ...TRAINING: 0.964109\n",
      "COST: 0.0405132\n",
      "TEST: [0.0018391573]\n",
      "epoch: 1601 ...1602 ...1603 ...1604 ...1605 ...1606 ...1607 ...1608 ...1609 ...1610 ...1611 ...1612 ...1613 ...1614 ...1615 ...1616 ...1617 ...1618 ...1619 ...1620 ...1621 ...1622 ...1623 ...1624 ...1625 ...1626 ...1627 ...1628 ...1629 ...1630 ...1631 ...1632 ...1633 ...1634 ...1635 ...1636 ...1637 ...1638 ...1639 ...1640 ...1641 ...1642 ...1643 ...1644 ...1645 ...1646 ...1647 ...1648 ...1649 ...1650 ...1651 ...1652 ...1653 ...1654 ...1655 ...1656 ...1657 ...1658 ...1659 ...1660 ...1661 ...1662 ...1663 ...1664 ...1665 ...1666 ...1667 ...1668 ...1669 ...1670 ...1671 ...1672 ...1673 ...1674 ...1675 ...1676 ...1677 ...1678 ...1679 ...1680 ...1681 ...1682 ...1683 ...1684 ...1685 ...1686 ...1687 ...1688 ...1689 ...1690 ...1691 ...1692 ...1693 ...1694 ...1695 ...1696 ...1697 ...1698 ...1699 ...1700 ...TRAINING: 0.964956\n",
      "COST: 0.0384021\n",
      "TEST: [0.0018057181]\n",
      "epoch: 1701 ...1702 ...1703 ...1704 ...1705 ...1706 ...1707 ...1708 ...1709 ...1710 ...1711 ...1712 ...1713 ...1714 ...1715 ...1716 ...1717 ...1718 ...1719 ...1720 ...1721 ...1722 ...1723 ...1724 ...1725 ...1726 ...1727 ...1728 ...1729 ...1730 ...1731 ...1732 ...1733 ...1734 ...1735 ...1736 ...1737 ...1738 ...1739 ...1740 ...1741 ...1742 ...1743 ...1744 ...1745 ...1746 ...1747 ...1748 ...1749 ...1750 ...1751 ...1752 ...1753 ...1754 ...1755 ...1756 ...1757 ...1758 ...1759 ...1760 ...1761 ...1762 ...1763 ...1764 ...1765 ...1766 ...1767 ...1768 ...1769 ...1770 ...1771 ...1772 ...1773 ...1774 ...1775 ...1776 ...1777 ...1778 ...1779 ...1780 ...1781 ...1782 ...1783 ...1784 ...1785 ...1786 ...1787 ...1788 ...1789 ...1790 ...1791 ...1792 ...1793 ...1794 ...1795 ...1796 ...1797 ...1798 ...1799 ...1800 ...TRAINING: 0.96667\n",
      "COST: 0.0366378\n",
      "TEST: [0.0017722789]\n",
      "epoch: 1801 ...1802 ...1803 ...1804 ...1805 ...1806 ...1807 ...1808 ...1809 ...1810 ...1811 ...1812 ...1813 ...1814 ...1815 ...1816 ...1817 ...1818 ...1819 ...1820 ...1821 ...1822 ...1823 ...1824 ...1825 ...1826 ...1827 ...1828 ...1829 ...1830 ...1831 ...1832 ...1833 ...1834 ...1835 ...1836 ...1837 ...1838 ...1839 ...1840 ...1841 ...1842 ...1843 ...1844 ...1845 ...1846 ...1847 ...1848 ...1849 ...1850 ...1851 ...1852 ...1853 ...1854 ...1855 ...1856 ...1857 ...1858 ...1859 ...1860 ...1861 ...1862 ...1863 ...1864 ...1865 ...1866 ...1867 ...1868 ...1869 ...1870 ...1871 ...1872 ...1873 ...1874 ...1875 ...1876 ...1877 ...1878 ...1879 ...1880 ...1881 ...1882 ...1883 ...1884 ...1885 ...1886 ...1887 ...1888 ...1889 ...1890 ...1891 ...1892 ...1893 ...1894 ...1895 ...1896 ...1897 ...1898 ...1899 ...1900 ...TRAINING: 0.969252\n",
      "COST: 0.0351757\n",
      "TEST: [0.0017722789]\n",
      "epoch: 1901 ...1902 ...1903 ...1904 ...1905 ...1906 ...1907 ...1908 ...1909 ...1910 ...1911 ...1912 ...1913 ...1914 ...1915 ...1916 ...1917 ...1918 ...1919 ...1920 ...1921 ...1922 ...1923 ...1924 ...1925 ...1926 ...1927 ...1928 ...1929 ...1930 ...1931 ...1932 ...1933 ...1934 ...1935 ...1936 ...1937 ...1938 ...1939 ...1940 ...1941 ...1942 ...1943 ...1944 ...1945 ...1946 ...1947 ...1948 ...1949 ...1950 ...1951 ...1952 ...1953 ...1954 ...1955 ...1956 ...1957 ...1958 ...1959 ...1960 ...1961 ...1962 ...1963 ...1964 ...1965 ...1966 ...1967 ...1968 ...1969 ...1970 ...1971 ...1972 ...1973 ...1974 ...1975 ...1976 ...1977 ...1978 ...1979 ...1980 ...1981 ...1982 ...1983 ...1984 ...1985 ...1986 ...1987 ...1988 ...1989 ...1990 ...1991 ...1992 ...1993 ...1994 ...1995 ...1996 ...1997 ...1998 ...1999 ..."
     ]
    }
   ],
   "source": [
    "print('epoch: ', end=\"\")\n",
    "for i in range(training_epochs):  \n",
    "    print(i, '...', end=\"\")\n",
    "    sess.run([optimizer], feed_dict={x_inputs: trainX, y_outputs: trainY, keep_prob: dropout_keep_percentage})\n",
    "    if (i) % display_step == 0:\n",
    "        train_acc, train_cost = sess.run([accuracy, cost], feed_dict={x_inputs: trainX, y_outputs: trainY, keep_prob: 1.0})\n",
    "        test_acc = sess.run([accuracy], feed_dict={x_inputs: testX, y_outputs: testY, keep_prob: 1.0})\n",
    "        print(\"TRAINING:\", train_acc)\n",
    "        print(\"COST:\", train_cost)\n",
    "        print(\"TEST:\", test_acc)\n",
    "        print('epoch: ', end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
